---
title: 
output: 
  html_document:
    toc: true
---

```{r}
library(jiebaR)
library(Rfacebook)
library(tidyverse)
library(tidytext)
library(wordcloud2)
library(readr)
library(dplyr)
library(devtools)
library(knitr)
library(jsonlite)
library(httr)
library(wordcloud2)

```

# Exploratory (Textual) Data Analysis

```{r}
token <- 'EAACEdEose0cBAK8bOlUZCjgeSXZCabMF0w31zdXUVKabzdoGZCZA7PjyJm6NUQ2S4ygnzlr0NHymPP0aZCUlADanNDZAiVhfrW6PgXYh2rUmrUwKZCHySopfZCqYNtuwVJ2dWSuKTvezZCnjytT2QQU57knUEFrS9pQN15Bzf4zNlQJTdzy1k9rZBcAryUZAiajkfssHEH9bd6wVwZDZD'


```



## Part 1
### 分析黑特台大Facebook粉專
 - 總資料(共擷取了2257篇貼文，從2015年2月~2017年11月)詞頻分析 <由於資料龐大跑很慢，請見補充說明檔>
 - 依照不同區間的按讚數、留言數、分享數做分組，計算各組擁有的貼文數。
 - 從先前的總資料分析中，發現有三種高頻詞「女友」、「我要」、「幹」，觀察此三詞在上面分類裡的出現頻率排名。

```{r}

hateNTU <- getPage("hateNTU", token, n=3000 )
#print(hateNTU)
#from_id
#from_name
#message
#created_time
#type link
#id
#story
hateNTU.message0 <- hateNTU$message[hateNTU$likes_count>0]
hateNTU.message01 <- hateNTU$message[hateNTU$likes_count>10]
hateNTU.message1 <- hateNTU$message[hateNTU$likes_count>50]
hateNTU.message2 <- hateNTU$message[hateNTU$likes_count>100]
hateNTU.message3 <- hateNTU$message[hateNTU$likes_count>200]
hateNTU.message4 <- hateNTU$message[hateNTU$likes_count>500]
hateNTU.message5 <- hateNTU$message[hateNTU$likes_count>1000]
#comments_count
hateNTU.message11 <- hateNTU$message[hateNTU$comments_count>50]
hateNTU.message12 <- hateNTU$message[hateNTU$comments_count>100]
hateNTU.message13 <- hateNTU$message[hateNTU$comments_count>200]
hateNTU.message14 <- hateNTU$message[hateNTU$comments_count>300]
hateNTU.message15 <- hateNTU$message[hateNTU$comments_count>400]
#shares_count
hateNTU.message21 <- hateNTU$message[hateNTU$shares_count>5]
hateNTU.message22 <- hateNTU$message[hateNTU$shares_count>10]
hateNTU.message23 <- hateNTU$message[hateNTU$shares_count>15]
hateNTU.message24 <- hateNTU$message[hateNTU$shares_count>20]
hateNTU.message25 <- hateNTU$message[hateNTU$shares_count>25]


# 各按讚數、回覆數、分享數的篇數
hateNTU.condition <- c( "likes_count>0" , "likes_count>10" ,
                        "likes_count>50"  ,
                        "likes_count>100" , "likes_count>200" ,
                        "likes_count>500" , "likes_count>1000" ,
                        "comments_count>50" , "comments_count>100" ,
                        "comments_count>200" , "comments_count>300" ,
                        "comments_count>400" , "shares_count>5" ,
                        "shares_count>10" , "shares_count>15" ,
                        "shares_count>20" , "shares_count>25" )
hateNTU.number <- c( length(hateNTU.message0) , length(hateNTU.message01) , 
                     length(hateNTU.message1) ,         
                     length(hateNTU.message2) , length(hateNTU.message3) , 
                     length(hateNTU.message4) , length(hateNTU.message5) , 
                     length(hateNTU.message11) , length(hateNTU.message12) , 
                     length(hateNTU.message13) , length(hateNTU.message14) , 
                     length(hateNTU.message15) , length(hateNTU.message21) , 
                     length(hateNTU.message22) , length(hateNTU.message23) , 
                     length(hateNTU.message24) , length(hateNTU.message25) )



# 把各篇擷取出來，並做斷詞
seg <- worker()
outputfile01   <- seg[hateNTU.message01]
outputfile10   <- seg[hateNTU.message1]
outputfile20   <- seg[hateNTU.message2]
outputfile30   <- seg[hateNTU.message3]
outputfile40   <- seg[hateNTU.message4]
outputfile50   <- seg[hateNTU.message5]

outputfile110   <- seg[hateNTU.message11]
outputfile120   <- seg[hateNTU.message12]
outputfile130   <- seg[hateNTU.message13]
outputfile140   <- seg[hateNTU.message14]
outputfile150   <- seg[hateNTU.message15]

outputfile210   <- seg[hateNTU.message21]
outputfile220   <- seg[hateNTU.message22]
outputfile230   <- seg[hateNTU.message23]
outputfile240   <- seg[hateNTU.message24]
outputfile250   <- seg[hateNTU.message25]



# 詞頻計算

outputfile0.2 <- data.frame(outputfile01)
outputfile0.3 <- table(unlist(outputfile0.2))
outputfile0.4 <- as.data.frame(outputfile0.3)
colnames(outputfile0.4) <- c("term", "frequency")
outputfile0.5 <- arrange(outputfile0.4 , desc(frequency))
d0 <- dim(outputfile0.5)


outputfile10.2 <- data.frame(outputfile10)
outputfile10.3 <- table(unlist(outputfile10.2))
outputfile10.4 <- as.data.frame(outputfile10.3)
colnames(outputfile10.4) <- c("term", "frequency")
outputfile10.5 <- arrange(outputfile10.4 , desc(frequency))
d1 <- dim(outputfile10.5)

outputfile20.2 <- data.frame(outputfile20)
outputfile20.3 <- table(unlist(outputfile20.2))
outputfile20.4 <- as.data.frame(outputfile20.3)
colnames(outputfile20.4) <- c("term", "frequency")
outputfile20.5 <- arrange(outputfile20.4 , desc(frequency))
d2 <- dim(outputfile20.5)

outputfile30.2 <- data.frame(outputfile30)
outputfile30.3 <- table(unlist(outputfile30.2))
outputfile30.4 <- as.data.frame(outputfile30.3)
colnames(outputfile30.4) <- c("term", "frequency")
outputfile30.5 <- arrange(outputfile30.4 , desc(frequency))
d3 <- dim(outputfile30.5)

outputfile40.2 <- data.frame(outputfile40)
outputfile40.3 <- table(unlist(outputfile40.2))
outputfile40.4 <- as.data.frame(outputfile40.3)
colnames(outputfile40.4) <- c("term", "frequency")
outputfile40.5 <- arrange(outputfile40.4 , desc(frequency))
d4 <- dim(outputfile40.5)

outputfile50.2 <- data.frame(outputfile50)
outputfile50.3 <- table(unlist(outputfile50.2))
outputfile50.4 <- as.data.frame(outputfile50.3)
colnames(outputfile50.4) <- c("term", "frequency")
outputfile50.5 <- arrange(outputfile50.4 , desc(frequency))
d5 <- dim(outputfile50.5)


outputfile110.2 <- data.frame(outputfile110)
outputfile110.3 <- table(unlist(outputfile110.2))
outputfile110.4 <- as.data.frame(outputfile110.3)
colnames(outputfile110.4) <- c("term", "frequency")
outputfile110.5 <- arrange(outputfile110.4 , desc(frequency))
d6 <- dim(outputfile110.5)

outputfile120.2 <- data.frame(outputfile120)
outputfile120.3 <- table(unlist(outputfile120.2))
outputfile120.4 <- as.data.frame(outputfile120.3)
colnames(outputfile120.4) <- c("term", "frequency")
outputfile120.5 <- arrange(outputfile120.4 , desc(frequency))
d7 <- dim(outputfile120.5)


outputfile130.2 <- data.frame(outputfile130)
outputfile130.3 <- table(unlist(outputfile130.2))
outputfile130.4 <- as.data.frame(outputfile130.3)
colnames(outputfile130.4) <- c("term", "frequency")
outputfile130.5 <- arrange(outputfile130.4 , desc(frequency))
d8 <- dim(outputfile130.5)

outputfile140.2 <- data.frame(outputfile140)
outputfile140.3 <- table(unlist(outputfile140.2))
outputfile140.4 <- as.data.frame(outputfile140.3)
colnames(outputfile140.4) <- c("term", "frequency")
outputfile140.5 <- arrange(outputfile140.4 , desc(frequency))
d9 <- dim(outputfile140.5)

outputfile150.2 <- data.frame(outputfile150)
outputfile150.3 <- table(unlist(outputfile150.2))
outputfile150.4 <- as.data.frame(outputfile150.3)
colnames(outputfile150.4) <- c("term", "frequency")
outputfile150.5 <- arrange(outputfile150.4 , desc(frequency))
d10 <- dim(outputfile150.5)


outputfile210.2 <- data.frame(outputfile210)
outputfile210.3 <- table(unlist(outputfile210.2))
outputfile210.4 <- as.data.frame(outputfile210.3)
colnames(outputfile210.4) <- c("term", "frequency")
outputfile210.5 <- arrange(outputfile210.4 , desc(frequency))
d11 <- dim(outputfile210.5)

outputfile220.2 <- data.frame(outputfile220)
outputfile220.3 <- table(unlist(outputfile220.2))
outputfile220.4 <- as.data.frame(outputfile220.3)
colnames(outputfile220.4) <- c("term", "frequency")
outputfile220.5 <- arrange(outputfile220.4 , desc(frequency))
d12 <- dim(outputfile220.5)

outputfile230.2 <- data.frame(outputfile230)
outputfile230.3 <- table(unlist(outputfile230.2))
outputfile230.4 <- as.data.frame(outputfile230.3)
colnames(outputfile230.4) <- c("term", "frequency")
outputfile230.5 <- arrange(outputfile230.4 , desc(frequency))
d13 <- dim(outputfile230.5)

outputfile240.2 <- data.frame(outputfile240)
outputfile240.3 <- table(unlist(outputfile240.2))
outputfile240.4 <- as.data.frame(outputfile240.3)
colnames(outputfile240.4) <- c("term", "frequency")
outputfile240.5 <- arrange(outputfile240.4 , desc(frequency))
d14 <- dim(outputfile240.5)

outputfile250.2 <- data.frame(outputfile250)
outputfile250.3 <- table(unlist(outputfile250.2))
outputfile250.4 <- as.data.frame(outputfile250.3)
colnames(outputfile250.4) <- c("term", "frequency")
outputfile250.5 <- arrange(outputfile250.4 , desc(frequency))
d15 <- dim(outputfile250.5)



hateNTU.c.n <- data.frame(hateNTU.condition=hateNTU.condition , hateNTU.number=hateNTU.number)
# 各按讚數、回覆數、分享數的篇數
kable(hateNTU.c.n)




ssttrr1 <- c("女友" , "我要" , "幹")
fire1 <- character(40)
fire2 <- character(40)
fire3 <- numeric(40)
fire4 <- numeric(40)

fire1[1] <- c("all messages")
fire1[2] <- c("all messages")
fire1[3] <- c("all messages")

fire2[1] <- c("女友")
fire2[2] <- c("我要")
fire2[3] <- c("幹")

fire3[1] <- c("682")
fire3[2] <- c("661")
fire3[3] <- c("403")

fire4[1] <- c("22")
fire4[2] <- c("23")
fire4[3] <- c("47")



counter10 <- 3
for ( i in 1:d0[1] ){
    for ( j in 1:3 ){
       if ( nchar(as.character(outputfile0.5[i,1])) == nchar(ssttrr1[j]) ){
          grl1 <- grepl(outputfile0.5[i,1] , ssttrr1[j])
          if ( grl1 == TRUE ){
            counter10 <- counter10 + 1
            fire1[counter10] <- c("likes_count>10")
            fire2[counter10] <- as.character(outputfile0.5[i,1])
            fire3[counter10] <- outputfile0.5[i,2]
            fire4[counter10] <- i
          }
       }
    }
}


for ( i in 1:d1[1] ){
    for ( j in 1:3 ){
       if ( nchar(as.character(outputfile10.5[i,1])) == nchar(ssttrr1[j]) ){
          grl1 <- grepl(outputfile10.5[i,1] , ssttrr1[j])
          if ( grl1 == TRUE ){
            counter10 <- counter10 + 1
            fire1[counter10] <- c("likes_count>50")
            fire2[counter10] <- as.character(outputfile10.5[i,1])
            fire3[counter10] <- outputfile10.5[i,2]
            fire4[counter10] <- i
          }
       }
    }
}


for ( i in 1:d2[1] ){
    for ( j in 1:3 ){
       if ( nchar(as.character(outputfile20.5[i,1])) == nchar(ssttrr1[j]) ){
          grl1 <- grepl(outputfile20.5[i,1] , ssttrr1[j])
          if ( grl1 == TRUE ){
            counter10 <- counter10 + 1
            fire1[counter10] <- c("likes_count>100")
            fire2[counter10] <- as.character(outputfile20.5[i,1])
            fire3[counter10] <- outputfile20.5[i,2]
            fire4[counter10] <- i
          }
       }
    }
}


for ( i in 1:d3[1] ){
    for ( j in 1:3 ){
       if ( nchar(as.character(outputfile30.5[i,1])) == nchar(ssttrr1[j]) ){
          grl1 <- grepl(outputfile30.5[i,1] , ssttrr1[j])
          if ( grl1 == TRUE ){
            counter10 <- counter10 + 1
            fire1[counter10] <- c("likes_count>200")
            fire2[counter10] <- as.character(outputfile30.5[i,1])
            fire3[counter10] <- outputfile30.5[i,2]
            fire4[counter10] <- i
          }
       }
    }
}


for ( i in 1:d4[1] ){
    for ( j in 1:3 ){
       if ( nchar(as.character(outputfile40.5[i,1])) == nchar(ssttrr1[j]) ){
          grl1 <- grepl(outputfile40.5[i,1] , ssttrr1[j])
          if ( grl1 == TRUE ){
            counter10 <- counter10 + 1
            fire1[counter10] <- c("likes_count>500")
            fire2[counter10] <- as.character(outputfile40.5[i,1])
            fire3[counter10] <- outputfile40.5[i,2]
            fire4[counter10] <- i
          }
       }
    }
}



for ( i in 1:d5[1] ){
    for ( j in 1:3 ){
       if ( nchar(as.character(outputfile50.5[i,1])) == nchar(ssttrr1[j]) ){
          grl1 <- grepl(outputfile50.5[i,1] , ssttrr1[j])
          if ( grl1 == TRUE ){
            counter10 <- counter10 + 1
            fire1[counter10] <- c("likes_count>1000")
            fire2[counter10] <- as.character(outputfile50.5[i,1])
            fire3[counter10] <- outputfile50.5[i,2]
            fire4[counter10] <- i
          }
       }
    }
}





for ( i in 1:d6[1] ){
    for ( j in 1:3 ){
       if ( nchar(as.character(outputfile110.5[i,1])) == nchar(ssttrr1[j]) ){
          grl1 <- grepl(outputfile110.5[i,1] , ssttrr1[j])
          if ( grl1 == TRUE ){
            counter10 <- counter10 + 1
            fire1[counter10] <- c("comments_count>50")
            fire2[counter10] <- as.character(outputfile110.5[i,1])
            fire3[counter10] <- outputfile110.5[i,2]
            fire4[counter10] <- i
          }
       }
    }
}



for ( i in 1:d7[1] ){
    for ( j in 1:3 ){
       if ( nchar(as.character(outputfile120.5[i,1])) == nchar(ssttrr1[j]) ){
          grl1 <- grepl(outputfile120.5[i,1] , ssttrr1[j])
          if ( grl1 == TRUE ){
            counter10 <- counter10 + 1
            fire1[counter10] <- c("comments_count>100")
            fire2[counter10] <- as.character(outputfile120.5[i,1])
            fire3[counter10] <- outputfile120.5[i,2]
            fire4[counter10] <- i
          }
       }
    }
}




for ( i in 1:d8[1] ){
    for ( j in 1:3 ){
       if ( nchar(as.character(outputfile130.5[i,1])) == nchar(ssttrr1[j]) ){
          grl1 <- grepl(outputfile130.5[i,1] , ssttrr1[j])
          if ( grl1 == TRUE ){
            counter10 <- counter10 + 1
            fire1[counter10] <- c("comments_count>200")
            fire2[counter10] <- as.character(outputfile130.5[i,1])
            fire3[counter10] <- outputfile130.5[i,2]
            fire4[counter10] <- i
          }
       }
    }
}




for ( i in 1:d9[1] ){
    for ( j in 1:3 ){
       if ( nchar(as.character(outputfile140.5[i,1])) == nchar(ssttrr1[j]) ){
          grl1 <- grepl(outputfile140.5[i,1] , ssttrr1[j])
          if ( grl1 == TRUE ){
            counter10 <- counter10 + 1
            fire1[counter10] <- c("comments_count>300")
            fire2[counter10] <- as.character(outputfile140.5[i,1])
            fire3[counter10] <- outputfile140.5[i,2]
            fire4[counter10] <- i
          }
       }
    }
}





for ( i in 1:d10[1] ){
    for ( j in 1:3 ){
       if ( nchar(as.character(outputfile150.5[i,1])) == nchar(ssttrr1[j]) ){
          grl1 <- grepl(outputfile150.5[i,1] , ssttrr1[j])
          if ( grl1 == TRUE ){
            counter10 <- counter10 + 1
            fire1[counter10] <- c("comments_count>400")
            fire2[counter10] <- as.character(outputfile150.5[i,1])
            fire3[counter10] <- outputfile150.5[i,2]
            fire4[counter10] <- i
          }
       }
    }
}




for ( i in 1:d11[1] ){
    for ( j in 1:3 ){
       if ( nchar(as.character(outputfile210.5[i,1])) == nchar(ssttrr1[j]) ){
          grl1 <- grepl(outputfile210.5[i,1] , ssttrr1[j])
          if ( grl1 == TRUE ){
            counter10 <- counter10 + 1
            fire1[counter10] <- c("shares_count>5")
            fire2[counter10] <- as.character(outputfile210.5[i,1])
            fire3[counter10] <- outputfile210.5[i,2]
            fire4[counter10] <- i
          }
       }
    }
}





for ( i in 1:d12[1] ){
    for ( j in 1:3 ){
       if ( nchar(as.character(outputfile220.5[i,1])) == nchar(ssttrr1[j]) ){
          grl1 <- grepl(outputfile220.5[i,1] , ssttrr1[j])
          if ( grl1 == TRUE ){
            counter10 <- counter10 + 1
            fire1[counter10] <- c("shares_count>10")
            fire2[counter10] <- as.character(outputfile220.5[i,1])
            fire3[counter10] <- outputfile220.5[i,2]
            fire4[counter10] <- i
          }
       }
    }
}




for ( i in 1:d13[1] ){
    for ( j in 1:3 ){
       if ( nchar(as.character(outputfile230.5[i,1])) == nchar(ssttrr1[j]) ){
          grl1 <- grepl(outputfile230.5[i,1] , ssttrr1[j])
          if ( grl1 == TRUE ){
            counter10 <- counter10 + 1
            fire1[counter10] <- c("shares_count>15")
            fire2[counter10] <- as.character(outputfile230.5[i,1])
            fire3[counter10] <- outputfile230.5[i,2]
            fire4[counter10] <- i
          }
       }
    }
}



for ( i in 1:d14[1] ){
    for ( j in 1:3 ){
       if ( nchar(as.character(outputfile240.5[i,1])) == nchar(ssttrr1[j]) ){
          grl1 <- grepl(outputfile240.5[i,1] , ssttrr1[j])
          if ( grl1 == TRUE ){
            counter10 <- counter10 + 1
            fire1[counter10] <- c("shares_count>20")
            fire2[counter10] <- as.character(outputfile240.5[i,1])
            fire3[counter10] <- outputfile240.5[i,2]
            fire4[counter10] <- i
          }
       }
    }
}




for ( i in 1:d15[1] ){
    for ( j in 1:3 ){
       if ( nchar(as.character(outputfile250.5[i,1])) == nchar(ssttrr1[j]) ){
          grl1 <- grepl(outputfile250.5[i,1] , ssttrr1[j])
          if ( grl1 == TRUE ){
            counter10 <- counter10 + 1
            fire1[counter10] <- c("shares_count>25")
            fire2[counter10] <- as.character(outputfile250.5[i,1])
            fire3[counter10] <- outputfile250.5[i,2]
            fire4[counter10] <- i
          }
       }
    }
}

fire1000 <- data.frame(fire1 , fire2 , fire3 , fire4)
colnames(fire1000) <- c("條件" , "詞" , "次數" , "在該條件下，出現的次數的排名")
kable(fire1000)


```

## Part 2
### 比較PTT熱門搜尋關鍵字與黑特台大，分析兩者是否相關。

```{r get_access_token}
login = jsonlite::toJSON(list(email = "b04502043@ntu.edu.tw", clientId = "5i48m5nk3j23ok64vgpv0udec0"), auto_unbox = TRUE)
r = POST("https://ser.kong.srm.pw/dashboard/token/authorize", content_type_json(), body = login)
resp = content(r)
access_token = resp[2]

```



```{r get_data}


url = paste("https://more.kong.srm.pw/topArticle/ptt?period=30&limit=100&api_key=", access_token, sep = "")
r = GET(url)
title = c("title", "board", "url", "comments", "dislike", "neutral", "like", "time")
resp = data.frame(content(r))
resp = resp[4:803]
result = data.frame(resp[1:8])
names(result) = title
l = length(resp)-7
i = 9
while(i <= l){
  #print(i)
  tmp = data.frame(resp[i:(i+7)])
  names(tmp) = title
  result = rbind(result, tmp)
  i <- i+8
}
result

```


# Part 3
## 政治人物在2017年8月間的發文與世界大學運動會的相關性
擷取總統及六都市長Facebook粉絲專頁資料並加以分析，觀察政治人物在2017年8月間的發文與世界大學運動會的相關性。由做出的文字雲中可看出”世大運”在8月的發文中出現頻率高，而且這些關鍵字的出現也吸引了相對多按讚數。
這部分還可以繼續做衍生至特殊事件發生期間與fb發文關鍵字的關聯性。


```{r part3}
str100 <- c("136845026417486","46251501064","232716627404","152472278103133","10150145806225128","333058400178329","153819538009272")
str200 <- as.character(str100)

lenstr200 <- length(str200)

fanpage1 <- getPage('136845026417486', token, n = 40, since = "2017/08/01",until = "2017/08/30")

fanpage1$from_name # show出粉絲專頁的名稱

fanpage1 <- as.data.frame(fanpage1)

for ( i in 2:lenstr200 ){
  fanpage_i<- as.data.frame(getPage(str200[i], token, n =40 ,since = "2017/08/01",until = "2017/08/30"))
  fanpage1<-rbind(fanpage1,fanpage_i)
}

#fix(fanpage1)

seg=worker("tag") 
new_user_word(seg,c("世大運","世界大學運動會","選手村"))
word<-seg[as.character(fanpage1$message)] 
word<-segment(fanpage1$message,seg)
tmp_df<-data.frame(word,names(word))
colnames(tmp_df)<-c("Word","POS")
tmp_df %>%
  group_by(Word,POS) %>%
  summarise(Frequency=n()) -> lfreq
pfreq<-arrange(lfreq,desc(Word,POS))
pfreq<-subset(lfreq,lfreq$POS == "n" | lfreq$POS == "nr" )
pfreq<-data.frame(pfreq$Word,pfreq$Frequency)
wd<-head(pfreq[order(pfreq[,2],decreasing = T),],500)
wordcloud2(wd,size=1,backgroundColor = "black")

```